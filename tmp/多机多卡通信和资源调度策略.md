1.	资源采集方面，改进为采集每个node的capacity（总设备数量）和allocatable（当前可用数量），存入mysql，然后提供接口给前端请求，这样前端就知道每个node的具体设备数量情况

2.	前端方面，对于GPU
a)	提交regular Job时，拿到全部节点中的最高capacity值，用于限制用户可以选择设备num的最大值；拿到所有节点的最高allocatable值，用于提示用户填写了超出allocatable的设备num时，Job将会处于排队。
b)	提交distribute Job时，
i.	可以手动输入node数量（num of node），node数量最高限制为节点的数量和。
ii.	选择每个node的设备数量（num of per device）。每个node的设备数量可选择项为1，2，4…2^N，其中N值满足表达式2^N<=所有节点的最大capacity值。

NPU的regular Job类型，设备num限制还是0，1，2，4，8。
NPU的distribute Job类型，每个node的设备数量（num of per device）还是默认8卡。



多机，多卡通信
--------------------------------------------------------------------------------------

1）	多机多NPU偶发性失败，这里之所以是偶发性的，原因是只有当 ai-worker恰巧被调度到master节点后，
此时才会因为 ai-ps与ai-worker不能通信 导致pod启动失败（其它机器都关闭了防火墙）

2）	多机任务要求ai-ps节点可以向ai-worker节点ssh免密码访问
参考：https://horovod.readthedocs.io/en/stable/running_include.html#failures-due-to-ssh-issues 



他们的调度策略是尽量减少碎片、优先占满一个HCCL分组（如0-3、4-7为两个分组），
1）根据空闲资源分配NPU并生成json文件；---此功能已更新至南京环境
2）灵活调度（可理解为调度优化），目前这部分其实没能较好适配HCCL分组策略（分配两个1卡的NPU任务，可能会落到不同的组别），按今早沟通如调整涉及代码较多，建议后续更新完后同步至南京环境，要不某些场景会使用受限。
如果平台通信端口只能支持80/443，不支持打开30000-49999端口字段，这应该不影响平台大部分功能。主要限制是用户将无法使用ssh连接pod，而只能通过Jupyter界面。这对一般用户，大概是可以接受的。

此问题还需各方配合 编译若干镜像实现，下午已开会讨论。
后续应搭配 私有仓库+版本管理+CICD实现 管理和部署，彻底解决此问题。

